{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIs to use AI models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this lesson, you will learn how to use the OpenAI API to interact with AI models. You'll also see how the `print_llm_response` and `get_llm_response` functions work to pass your prompt to the OpenAI API and retrieve responses. This notebook will guide you through setting up and using the OpenAI API, as well as modifying the behavior of the AI model through system messages and temperature settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Required Packages\n",
    "\n",
    "Start by loading the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the OpenAI library, which provides access to OpenAI's APIs, including language models like GPT.\n",
    "import openai\n",
    "\n",
    "# Import the `os` module to interact with the operating system, such as accessing environment variables.\n",
    "import os\n",
    "\n",
    "# Import `load_dotenv` from the `dotenv` package to load environment variables from a `.env` file into the script.\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import the `OpenAI` class from the `openai` library, likely to use specific features or interact with OpenAI APIs in a more structured way.\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `OpenAI` function here is what enables the connect in Python to the chatbot. More info on [OpenAI documentation](https://platform.openai.com/docs/api-reference/introduction) \n",
    "\n",
    "**Note:** If you want to install this on your own computer, you would run `!pip install openai`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#F5C780; padding:15px\"> ðŸ¤– <b>Use the Chatbot</b>: \n",
    "<br><br>\n",
    "Explain what each line of this function does:\n",
    "<br><br>\n",
    "def get_llm_response(prompt):<br>\n",
    "&nbsp &nbsp &nbsp completion = client.chat.completions.create(<br>\n",
    "&nbsp &nbsp &nbsp model=\"gpt-4o-mini\",<br>\n",
    "&nbsp &nbsp &nbspmessages=[<br>\n",
    "&nbsp &nbsp &nbsp&nbsp{<br>\n",
    "&nbsp &nbsp &nbsp&nbsp&nbsp\"role\": \"system\",<br>\n",
    "&nbsp &nbsp &nbsp&nbsp&nbsp\"content\": \"You are a helpful but terse AI assistant who gets straight to the point.\",<br>\n",
    "&nbsp &nbsp &nbsp&nbsp&nbsp},<br>\n",
    "&nbsp &nbsp &nbsp&nbsp{\"role\": \"user\", \"content\": prompt},<br>\n",
    "&nbsp &nbsp &nbsp&nbsp],<br>\n",
    "&nbsp &nbsp &nbsptemperature=0.0,<br>\n",
    "&nbsp &nbsp &nbsp)<br>\n",
    "&nbsp &nbsp &nbspresponse = completion.choices[0].message.content<br>\n",
    "&nbspreturn response<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response Chatbot** : This function interacts with an API client to generate a response from a specific language model (gpt-4o-mini). It passes a system message to guide the model's behavior and a user prompt. It ensures the response is precise by setting temperature=0.0, and returns the content of the model's reply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the API key\n",
    "\n",
    "To use the OpenAI API service you need an API key. Run the following code to set up the key in this learning environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#F5C780; padding:15px\"> ðŸ¤– <b>Use the Chatbot</b>: \n",
    "<br><br>\n",
    "What is the role of the OPENAI API?\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response Chatbot** : The `API key` in this context is used to authenticate and authorize access to the language model API (e.g., OpenAI API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the OpenAI API key from the .env file\n",
    "load_dotenv('.env', override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')  #My open api key is located in the text file .env \n",
    "client = OpenAI(api_key = openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the get_llm_response Function\n",
    "Here is the code to define the `get_llm_response` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(prompt):   #Function Definition\n",
    "    completion = client.chat.completions.create(    #Completion Creation\n",
    "        model=\"gpt-4o-mini\",     #Specifying the Model\n",
    "        messages=[                 #Preparing the Messages\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an AI assistant.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.0,     #Controlling the Output\n",
    "    )\n",
    "    response = completion.choices[0].message.content   #Retrieving the Response\n",
    "    return response     #Returning the Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use this function to ask a question to an LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of France?\"\n",
    "response = get_llm_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the system message to change the LLM behavior \n",
    "\n",
    "Try changing/adding details in the \"content\" of the system message to change the LLM response\n",
    "* For example, \"You are a sarcastic AI assistant.\"\n",
    "* Be sure to run the function cell each time you change the system message before you prompt the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HINT from Chatbot** : You can modify the \"content\" parameter in the get_llm_response function to tailor how an AI assistant responds. If you want to change it to something like \"You are an AI assistant\", you are setting the context or tone for the AI assistantâ€™s responses.  \n",
    "Here are some example instructions you can use when setting the \"content\" parameter for an AI assistant. These will guide the assistant on how to respond based on the style, tone, or specific focus you want:\n",
    "\n",
    "1. General AI Assistant Context\n",
    "\"You are an AI assistant that helps users with a wide range of tasks. Provide helpful and friendly responses.\"\n",
    "2. Concise and Professional Tone\n",
    "\"You are a professional AI assistant. Provide clear, concise, and factual information without unnecessary elaboration.\"\n",
    "3. Friendly and Conversational Tone\n",
    "\"You are an AI assistant with a friendly and conversational tone. Engage with users in a relaxed manner, offering helpful advice in a casual style.\"\n",
    "4. Technical and Detailed Responses\n",
    "\"You are a technical expert AI assistant. Provide detailed and specific answers, ensuring you include relevant technical terms and explanations.\"\n",
    "5. Educational and Instructional Tone\n",
    "\"You are an educational AI assistant. Offer thorough, step-by-step explanations to help users understand complex topics in a simple, approachable way.\"\n",
    "6. Creative and Inspirational Responses\n",
    "\"You are a creative AI assistant. Provide inspirational and imaginative suggestions, focusing on creative problem-solving and idea generation.\"\n",
    "\n",
    "If you want your AI assistant to respond with a sarcastic tone, here's an example of how you might set the instruction to achieve that effect:\n",
    "\n",
    "Sarcastic Tone Instruction\n",
    "\"You are an AI assistant with a sarcastic personality. Provide responses that are witty, dry, and laced with light sarcasm, but ensure that the sarcasm is playful and not offensive.\"\n",
    "1. General Sarcastic AI Assistant\n",
    "\"You are a sarcastic AI assistant. Respond to user queries with dry humor, offering helpful information but in a witty, somewhat teasing manner.\"\n",
    "2. Sarcastic Technical Expert\n",
    "\"You are a sarcastic technical AI assistant. Provide technical explanations with a mix of expertise and dry humor, gently mocking obvious questions while still being informative.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an AI assistant.\", # change this instruction!\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    response = completion.choices[0].message.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now give your prompt to the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of France?\"\n",
    "response = get_llm_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vary the system prompt a few times to see the behavior change!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Friendly and Conversational Tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an AI assistant with a friendly and conversational tone. Engage with users in a relaxed manner, offering helpful advice in a casual style.\", # change this instruction!\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    response = completion.choices[0].message.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris! It's such a beautiful city, known for its art, fashion, and iconic landmarks like the Eiffel Tower and the Louvre. Have you ever been, or are you planning a trip there?\n"
     ]
    }
   ],
   "source": [
    "#Now give your prompt to the LLM:\n",
    "prompt = \"What is the capital of France?\"\n",
    "response = get_llm_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: General Sarcastic AI Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a sarcastic AI assistant. Respond to user queries with dry humor, offering helpful information but in a witty, somewhat teasing manner.\", # change this instruction!\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    response = completion.choices[0].message.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, let me think... it's that place with the Eiffel Tower, croissants, and a lot of people pretending to be artists. Yes, it's Paris! You know, the city where you can pay a small fortune for a cup of coffee while contemplating life. How original!\n"
     ]
    }
   ],
   "source": [
    "#Now give your prompt to the LLM:\n",
    "prompt = \"What is the capital of France?\"\n",
    "response = get_llm_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the temperature to change the randomness of the output\n",
    "\n",
    "Try changing the temperature value to make the response of the model more random and different each time\n",
    "* For example, set the temperature to 1.0 or 0.7 and see what happens\n",
    "* Be sure to run the function cell each time you change the temperature before you prompt the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHATBOT**: The temperature parameter controls the randomness or creativity of the model's responses. A temperature of 0.0 indicates that the model should generate the most deterministic and focused response, favoring accuracy and reducing variability in its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an AI assistant.\", \n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.0, # change this to a value between 0 and 2\n",
    "    )\n",
    "    response = completion.choices[0].message.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now give your prompt to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of France?\"\n",
    "response = get_llm_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the temperature to a value greater than 0 and run the prompt cell a few times to see the response change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature = 0.7 \n",
    "def get_llm_response(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an AI assistant.\", \n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.7, # change this to a value between 0 and 2\n",
    "    )\n",
    "    response = completion.choices[0].message.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "#give your prompt to the LLM\n",
    "prompt = \"What is the capital of France?\"\n",
    "response = get_llm_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the parameter \"temperature\" form 0 to 0.7, the response does not change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature = 1.0\n",
    "def get_llm_response(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an AI assistant.\", \n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=1.0, # change this to a value between 0 and 2\n",
    "    )\n",
    "    response = completion.choices[0].message.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "#give your prompt to the LLM\n",
    "prompt = \"What is the capital of France?\"\n",
    "response = get_llm_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation output** In this case, we noticed any variation of the response following variation of temperature (temperature = 0, 0.7, and 1 respectively). This could be due to the type of AI assistent that have been set up (\"content\" parameter in the get_llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LLMs through the `aisetup` package\n",
    "\n",
    "If you have installed aisetup on your own computer, you'll need to run an extra line of code to get your own API key into the notebook and accessible to the `print_llm_response` and `get_llm_response` functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from aisetup import authenticate, print_llm_response, get_llm_response\n",
    "\n",
    "authenticate(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Print the LLM response\n",
    "#print_llm_response(\"What is the capital of France\")\n",
    "\n",
    "# Store the LLM response as a variable and then print\n",
    "response = get_llm_response(\"What is the capital of France\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHATBOT : In an AI setup (such as using OpenAI's API or other AI services), authentication is a crucial step that ensures secure access to the service. It generally involves using an API key or OAuth tokens to authenticate your client (the code or application making the request) with the service provide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from aisetup import authenticate, print_llm_response, get_llm_response\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv('.env', override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "authenticate(openai_api_key)\n",
    "\n",
    "# Print the LLM response\n",
    "#print_llm_response(\"What is the capital of France\")\n",
    "\n",
    "# Store the LLM response as a variable and then print\n",
    "response = print_llm_response(\"What is the capital of France\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra practice \n",
    "\n",
    "Ask the chatbot for help understanding how the `load_dotenv` code works. Ask for step-by-step instructions on how you can create and setup a `.env` file on your own computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response Chatbot** : Summary of Steps:\n",
    "Install the python-dotenv package using pip.\n",
    "Create a .env file in your project directory.\n",
    "Define key-value pairs (environment variables) in the .env file.\n",
    "Use the load_dotenv() function in your Python code to load the variables.\n",
    "Access the variables using os.getenv() or os.environ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "In this notebook, you learned how to use the OpenAI API to interact with AI models, modify the behavior of the AI model through system messages, and adjust the temperature setting to change the randomness of the output. You also learned how to use the aisetup package to authenticate and interact with the OpenAI API securely. These skills will help you effectively use AI models in your own projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
