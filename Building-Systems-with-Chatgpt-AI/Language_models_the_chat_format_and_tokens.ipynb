{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae5bcee9-6588-4d29-bbb9-6fb351ef6630",
   "metadata": {},
   "source": [
    "# Language Models, the Chat Format and Tokens  üåê "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c14b2d5",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook demonstrates how to `use OpenAI's language models`, particularly the gpt-3.5-turbo model, to create chat completions and manage tokens. We will cover the setup process, define helper functions, and provide examples of prompting the model to get completions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c797991-8486-4d79-8c1d-5dc0c1289c2f",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Load the API key and relevant Python libaries. For more info see [SETUP Instructions](https://github.com/16032022/DeepLearningAI-OpenAI-projects/blob/main/SETUP.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19cd4e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ba0938-7ca5-46c4-a9d1-b55708d4dc7c",
   "metadata": {},
   "source": [
    "#### helper function\n",
    "Throughout this course, we will use OpenAI's `gpt-3.5-turbo` model and the [chat completions endpoint](https://platform.openai.com/docs/guides/chat). This helper function will make it easier to use prompts and look at the generated outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01120be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI()\n",
    "\n",
    "# Define a function to get chat completions\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0  #This is the degree of randomness of the model's output \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55983d1",
   "metadata": {},
   "source": [
    "### Example Usage\n",
    "#### Prompt the model and get a completion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4022556e",
   "metadata": {},
   "source": [
    "- Example prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1cc57b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Example prompt to get a completion\n",
    "response = get_completion(\"What is the capital of France?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83d4e38-3e3c-4c5a-a949-040a27f29d63",
   "metadata": {},
   "source": [
    "#### Tokens\n",
    "- Example to demonstrate how the model handles tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc2d9e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "C:\\Users\\Michela\\AppData\\Local\\Temp\\ipykernel_5756\\569481024.py:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  response = get_completion(\"Take the letters in lollipop\\ and reverse them\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pilpolol\n"
     ]
    }
   ],
   "source": [
    "# Example to reverse the letters in a word\n",
    "response = get_completion(\"Take the letters in lollipop\\ and reverse them\")\n",
    "print(response)  # Expected output: \"popillol\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e810ca93",
   "metadata": {},
   "source": [
    "Improved Prompting : Adjusting the prompt to improve the output, changing the position of \\ in the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37cab84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-o-p-i-l-l-o-l\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"\"\"Take the letters in \\\n",
    "l-o-l-l-i-p-o-p and reverse them\"\"\")\n",
    "print(response) # Expected output: \"p-o-p-i-l-l-o-l\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b88940-d3ab-4c00-b5c0-31531deaacbd",
   "metadata": {},
   "source": [
    "### Helper function (chat format)\n",
    "Here's the helper function we'll use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df3e8c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI()\n",
    "\n",
    "# Define a function to get chat completions\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\", temperature=0,max_tokens=500):\n",
    "    #messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    #messages = [{\"role\": \"system\", \"content\": prompt}, {\"role\": \"user\", \"content\": prompt}]\n",
    "    messages =  [  \n",
    "{'role':'system', \n",
    " 'content':\"\"\"You are an assistant who\\\n",
    " #responds in the style of Dr Seuss.\"\"\"},    \n",
    "{'role':'user', \n",
    " 'content':\"\"\"write me a very short poem\\\n",
    " #about a happy carrot\"\"\"},  \n",
    "] \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens, \n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1041ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh the happy little carrot, so orange and bright,\n",
      "Growing in the garden, reaching for the light.\n",
      "With a big, leafy top and a crunchy delight,\n",
      "This carrot brings joy from morning 'til night.\n"
     ]
    }
   ],
   "source": [
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content':\"\"\"You are an assistant who\\\n",
    " responds in the style of Dr Seuss.\"\"\"},    \n",
    "{'role':'user', \n",
    " 'content':\"\"\"write me a very short poem\\\n",
    " about a happy carrot\"\"\"},  \n",
    "] \n",
    "response = get_completion(messages, temperature=1)\n",
    "#response = get_completion(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc88471",
   "metadata": {},
   "source": [
    "### length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8f6d1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI()\n",
    "\n",
    "\n",
    "# Define a function to get chat completions\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\", temperature=0,max_tokens=500):\n",
    "    #messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    #messages = [{\"role\": \"system\", \"content\": prompt}, {\"role\": \"user\", \"content\": prompt}]\n",
    "    messages =  [  \n",
    "{'role':'system',\n",
    " 'content':'All your responses must be \\\n",
    "one sentence long.'},    \n",
    "{'role':'user',\n",
    " 'content':'write me a story about a happy carrot'},  \n",
    "] \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens, \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32b9eeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once there was a cheerful carrot named Chester who always brightened up the garden with his contagious smile.\n"
     ]
    }
   ],
   "source": [
    "# length\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':'All your responses must be \\\n",
    "one sentence long.'},    \n",
    "{'role':'user',\n",
    " 'content':'write me a story about a happy carrot'},  \n",
    "] \n",
    "\n",
    "response = get_completion(messages, temperature=1)\n",
    "#response = get_completion(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbab1b9",
   "metadata": {},
   "source": [
    "### combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5415bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI()\n",
    "\n",
    "# Define a function to get chat completions\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\", temperature=0,max_tokens=500):\n",
    "    #messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    #messages = [{\"role\": \"system\", \"content\": prompt}, {\"role\": \"user\", \"content\": prompt}]\n",
    "    messages =  [  \n",
    "{'role':'system',\n",
    " 'content':\"\"\"You are an assistant who \\\n",
    "responds in the style of Dr Seuss. \\\n",
    "All your responses must be one sentence long.\"\"\"},    \n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a story about a happy carrot\"\"\"},\n",
    "] \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens, \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf029d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a vegetable patch so bright and cheery, there lived a carrot named Larry, always so merry.\n"
     ]
    }
   ],
   "source": [
    "# combined\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':\"\"\"You are an assistant who \\\n",
    "responds in the style of Dr Seuss. \\\n",
    "All your responses must be one sentence long.\"\"\"},    \n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a story about a happy carrot\"\"\"},\n",
    "] \n",
    "\n",
    "response = get_completion(messages, temperature=1)\n",
    "#response = get_completion(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce979420",
   "metadata": {},
   "source": [
    "### Token Dictionary Function (token_dict)\n",
    "Define a function to extract token usage information from the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8ef00ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI()\n",
    "\n",
    "# Define a function to get chat completions\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\", temperature=0,max_tokens=500):\n",
    "    #messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    #messages = [{\"role\": \"system\", \"content\": prompt}, {\"role\": \"user\", \"content\": prompt}]\n",
    "    messages =  [  \n",
    "{'role':'system', \n",
    " 'content':\"\"\"You are an assistant who\\\n",
    " responds in the style of Dr Seuss.\"\"\"},    \n",
    "{'role':'user', \n",
    " 'content':\"\"\"write me a very short poem\\\n",
    " about a happy carrot\"\"\"},  \n",
    "] \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens, \n",
    "    )\n",
    "\n",
    " \n",
    "    #return response.choices[0].message.content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6ae986e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, the happy carrot, so bright and so orange,\n",
      "In the garden it grows, a joyful storage.\n",
      "With a leafy green top and a crunchy bite,\n",
      "It brings smiles to all, such a delightful sight!\n"
     ]
    }
   ],
   "source": [
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content':\"\"\"You are an assistant who\\\n",
    " responds in the style of Dr Seuss.\"\"\"},    \n",
    "{'role':'user', \n",
    " 'content':\"\"\"write me a very short poem\\\n",
    " about a happy carrot\"\"\"},  \n",
    "] \n",
    "\n",
    "#response, token_dict= get_completion(messages)\n",
    "response = get_completion(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32db94f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-AstM5S99XQQYRpQiSQHAIPGyyzHHC', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Oh, the happy carrot, orange and bright,\\nGrown in the garden, a joyful sight.\\nWith a leafy green top, always grinning,\\nIn the soil, it loves spinning.\\nSo sweet and crunchy, a delight to eat,\\nThe happy carrot is always a treat!', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1737645861, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=58, prompt_tokens=35, total_tokens=93, prompt_tokens_details={'cached_tokens': 0, 'audio_tokens': 0}, completion_tokens_details={'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}))\n",
      "{'prompt_tokens': 35, 'completion_tokens': 58, 'total_tokens': 93}\n"
     ]
    }
   ],
   "source": [
    "# Define the token_dict function to extract token usage information\n",
    "def token_dict(response):\n",
    "    return {\n",
    "        'prompt_tokens': response.usage.prompt_tokens,\n",
    "        'completion_tokens': response.usage.completion_tokens,\n",
    "        'total_tokens': response.usage.total_tokens,\n",
    "    }\n",
    "\n",
    "# Set your OpenAI API key from environment variable\n",
    "#openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Example API call using `gpt-3.5-turbo` or `gpt-4` with client object\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",  # or \"gpt-4\"\n",
    "    messages=[\n",
    "{'role':'system', 'content':\"\"\"You are an assistant who responds in the style of Dr Seuss.\"\"\"},    \n",
    "{'role':'user', 'content':\"\"\"write me a very short poem about a happy carrot\"\"\"},  \n",
    "    ],\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "# Print the entire response to see what is returned\n",
    "print(response)\n",
    "\n",
    "# Ensure that 'usage' exists in the response to avoid potential KeyError\n",
    "if response.usage:\n",
    "    # Extract token info using the token_dict function\n",
    "    token_info = token_dict(response)\n",
    "    print(token_info)\n",
    "else:\n",
    "    print(\"No usage data available in the response.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98255b7d",
   "metadata": {},
   "source": [
    "**Explanation output** This outputIt reflects a successful API call producing a playful creative output.In detalis, it represents a response generated by OpenAI's ChatCompletion API, specifically using the gpt-3.5-turbo-0125 model. Here's a short breakdown:  \n",
    "_Generated Content:_ A lighthearted poem about a happy carrot, celebrating its vibrant colors, crunchiness, and its role in meals.\n",
    "_Metadata:_  \n",
    " - Completion Tokens: 81 tokens were used for the generated poem.  \n",
    " - Prompt Tokens: 35 tokens were used in the input prompt.\n",
    " - Total Tokens: 116 tokens combined (prompt + completion).    \n",
    "\n",
    "_Model Details:_ The response came from the \"gpt-3.5-turbo-0125\" model with a stop finish reason (indicating the response completed naturally).  \n",
    "_Usage Details:_ The metadata tracks token usage for billing and optimization purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7958681c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook provided an overview of how to use `OpenAI's language models to generate chat completions and manage tokens`. We defined helper functions, demonstrated example prompts, and extracted token usage information. These tools and techniques can be further extended and customized for various natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294dcc54-f941-422e-8602-d8e78a0da093",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
